---
title: "Model Analysis"
author: "J. Bravo"
date: "2024-01-09"
output: html_document
---
Notebook to compare different models. Objective is to determine the ideal statistical learning model by comparing the performance metrics of four different classification models, specifically, Random Forest (RF), Naive Bayes (NB), k-Nearest Neighbor (kNN) and Support Vector Machines (SVM).

## Load Packages and Data
Load the necessary libraries and set seed.
```{r}
library(dplyr) # for functions
library(data.table)
library(janitor)
library(randomForest)
library(tidyverse)
library(tidymodels)
tidymodels_prefer()
library(themis)
library(doParallel) # All operating systems
library(kernlab) # For SVM model
set.seed(420)  # Setting seed
```
Notice use of relative directory. Will use zip ziles and uncompress.
```{r}
# Commands to unzip a data file
zip_file <- "data/mont_cty_crash_reporting_incidents_data.zip"
out_dir <- "unzipfolder"
dir.create(file.path(".", out_dir), showWarnings = TRUE)
unzip(zip_file, exdir = out_dir)
```
Load the data.
```{r}
raw_data <- fread("../unzipfolder/mont_cty_crash_reporting_incidents_data.csv")
head(raw_data)
```
## Data Pre-Processing
The exploratory data analysis notebook reveals a need to remove empty strings, N/A's, unknown, and other categories in the applicable fields.
Recall the relevant features:
  Independent/Predictor Variables:
  * Weather
  * Light
  * Vehicle Body Type
  * Speed Limit
  * Latitude
  * Longitude
  Dependent/Outcome Variable:
  * Injury Severity
Next, create the initial dataframe that requires some so-called data cleaning.
```{r}
initial_df <- raw_data %>%
  select(
    "Weather",
    "Light",
    "Vehicle Body Type",
    "Speed Limit",
    #"Latitude",
    #"Longitude",
    "Injury Severity"
  ) %>%
  filter(
    # Weather
    raw_data$"Weather" != "OTHER",
    raw_data$"Weather" != "N/A",
    raw_data$"Weather" != "UNKNOWN",
    # Light
    raw_data$"Light" != "N/A",
    raw_data$"Light" != "OTHER",
    raw_data$"Light" != "UNKNOWN",
    # Vehicle Body Type
    raw_data$"Vehicle Body Type" != "N/A",
    raw_data$"Vehicle Body Type" != "OTHER",
    raw_data$"Vehicle Body Type" != "UNKNOWN",
    raw_data$"Vehicle Body Type" != ""
  ) %>%
  # Converting all the character data types to factors
  mutate_if(is.character, as.factor) %>%
  # Convert the only integer, i.e. speed limit, to factor
  # Will treat speed limit as a categorical variable
  mutate_if(is.integer, as.factor) %>%
  # Change the column names to snake and lower case
  clean_names()
head(initial_df)
```
Want to see the number of records in this new dataframe.
```{r}
nrow(initial_df)
```
Compute the difference in rows from raw data to cleaned data.
```{r}
records_diff <- nrow(raw_data) - nrow(initial_df)
records_diff
```
So there is a difference of about 20k records.
Compute the proportion of rows that are excluded from the clean data frame.
```{r}
proportion_excluded <- records_diff/nrow(raw_data)
proportion_excluded
```
So about 12% of the unprocessed data frame is remove from the cleaned data frame.

## Get a Subset
For testing purposes, will use a 2.5% sample of the new data set.
```{r}
num_records <- nrow(initial_df)
sample_size <- ceiling(num_records * 0.025)
subset_df <- initial_df[sample(1:num_records, sample_size, replace = FALSE),]
head(subset_df)
```
## Create splits, folds, performance metrics, and control object
```{r}
# Splits
splits <- initial_split(subset_df, prop = 0.75, strata = injury_severity)
train_split <- training(splits)
test_split <- testing(splits)
# Folds
severity_folds <- vfold_cv(train_split, v = 10, strata = injury_severity)
# Performance metrics - classification model
perf_metrics <- metric_set(roc_auc, recall, precision, f_meas)
# Control Object
ctrl_obj <- control_grid(verbose = FALSE, save_pred = TRUE)
```
## Configurations
```{r}
# Cluster
all_cores <- parallel::detectCores(logical = FALSE)
cluster <- makePSOCKcluster(all_cores)
```
## Prototype the Random Forest Model
Step 1a: Recipe
```{r}
rf_rec <-
  recipe(injury_severity  ~ ., data = train_split) %>%
  step_upsample(injury_severity, over_ratio = 1.0)
rf_rec
```
Step 1b: Model Specifications
```{r}
rf_model <-
  rand_forest(
      mtry = tune(),
      trees = tune(),
      min_n = tune()
      ) %>%
  set_mode("classification") %>%
  set_engine("randomForest")
rf_model
```
Step 1c: Workflow
```{r}
rf_wflow <-
  workflow() %>%
  add_model(rf_model) %>%
  add_recipe(rf_rec)
rf_wflow
```
Step 1d: Executing Tune Grid
```{r}
registerDoParallel(cluster)
# Begin timing for this model
start_time <- Sys.time()
rf_initial <-
  rf_wflow %>%
  tune_grid(
    resamples = severity_folds,
    metrics = perf_metrics,
    control = ctrl_obj
  )
end_time <- Sys.time()
end_time - start_time
rf_initial
stopCluster(cluster)
```
Step 1e: Collect performance metrics
```{r}
rf_perf_metrics <- collect_metrics(rf_initial)
rf_perf_metrics
```
autoplot
```{r}
autoplot(rf_initial) +
  scale_color_viridis_d(direction = -1) +
  theme(legend.position = "top")
```
F1 Score
```{r}
show_best(rf_initial, metric = "f_meas")
```
Precision
```{r}
show_best(rf_initial, metric = "precision")
```
Recall
```{r}
show_best(rf_initial, metric = "recall")
```
ROC-AUC
```{r}
show_best(rf_initial, metric = "roc_auc")
```
## Prototype the Support Vector Machine (SVM) Model
Step 2a: Initiate the recipe
```{r}
# Recipe
svm_recipe <-
  recipe(injury_severity  ~ ., data = train_split) %>%
  step_dummy(weather, light, vehicle_body_type) %>%
  step_upsample(injury_severity, over_ratio = 1.0)
svm_recipe
```
Step 2b: SVM Model Specifications
```{r}
svm_model <- svm_rbf(
  cost = tune(),
  rbf_sigma = tune()
  ) %>%
  set_mode("classification") %>%
  set_engine("kernlab")
svm_model
```
Step 2c: Create workflow
```{r}
svm_wflow <-
  workflow() %>%
  add_model(svm_model) %>%
  add_recipe(svm_recipe)
svm_wflow
```
Step 2d: Executing Grid
```{r}
registerDoParallel(cluster)
# Begin timing for this model
start_time <- Sys.time()
perf_metrics <- metric_set(roc_auc, recall, precision, f_meas)
svm_initial <-
  svm_wflow %>% 
  tune_grid(
    resamples = severity_folds,
    metrics = perf_metrics,
    control = ctrl_obj
  )
end_time <- Sys.time()
end_time - start_time
svm_initial
stopCluster(cluster)
```
Step 2e: Collect performance metrics
```{r}
svm_perf_metrics <- collect_metrics(svm_initial)
svm_perf_metrics
```
AutoPlot
```{r}
autoplot(svm_initial) +
  scale_color_viridis_d(direction = -1) +
  theme(legend.position = "top")
```
F1 Score
```{r}
show_best(svm_initial, metric = "f_meas")
```
Precision
```{r}
show_best(svm_initial, metric = "precision")
```
Recall
```{r}
show_best(svm_initial, metric = "recall")
```
ROC-AUC
```{r}
show_best(svm_initial, metric = "roc_auc")
```
## Prototype the k-Nearest Neighbor model
Step 3a: Recipe
```{r}
knn_recipe <-
  recipe(injury_severity  ~ ., data = train_split) %>%
  step_dummy(weather, light, vehicle_body_type) %>%
  step_upsample(injury_severity, over_ratio = 1.0)
knn_recipe
```
Step 3b: Model Specifications
```{r}
library(kknn)
knn_model <- nearest_neighbor(
  neighbors = tune(),
  weight_func = tune(),
  dist_power = tune(),
  mode = "classification",
  engine = "kknn"
  )
knn_model
```
Step 3c: Workflow
```{r}
knn_wflow <-
  workflow() %>%
  add_model(knn_model) %>%
  add_recipe(knn_recipe)
knn_wflow
```
Step 3d: Create Tuning Grid
```{r}
registerDoParallel(cluster)
# Begin timing for this model
start_time <- Sys.time()
# Grid Tuning
knn_initial <-
  knn_wflow %>%
  tune_grid(
    resamples = severity_folds,
    metrics = perf_metrics,
    control = ctrl_obj
  )
end_time <- Sys.time()
end_time - start_time
knn_initial
stopCluster(cluster)
```
Step 3e: Collect performance metrics
```{r}
knn_perf_metrics <- collect_metrics(knn_initial)
knn_perf_metrics
```
AutoPlot - show the performance profile across tuning parameters
```{r}
autoplot(knn_initial) +
  scale_color_viridis_d(direction = -1) +
  theme(legend.position = "top")
```
F1 Score
```{r}
show_best(knn_initial, metric = "f_meas")
```
Precision
```{r}
show_best(knn_initial, metric = "precision")
```
Recall
```{r}
show_best(knn_initial, metric = "recall")
```
ROC-AUC
```{r}
show_best(knn_initial, metric = "roc_auc")
```
## Prototype the Naive Bayes Model
Step 4a: Initiate the recipe
```{r}
# Recipe
nb_rec <-
  recipe(injury_severity  ~ ., data = train_split) %>%
  step_dummy(weather, light, vehicle_body_type) %>%
  step_upsample(injury_severity, over_ratio = 1.0)
nb_rec
```
Step 4b: Naive Bayes Model Specifications
```{r}
naive_bayes_model <- naive_Bayes(
  mode = "classification",
  smoothness = tune(),
  Laplace = tune(),
  engine = "naivebayes"
)
naive_bayes_model
```
Step 4c: Create workflow
```{r}
nb_model_wflow <-
  workflow() %>%
  add_model(naive_bayes_model) %>%
  add_recipe(nb_rec)
nb_model_wflow
```
Step 4d: Re-sampling
```{r}
perf_metrics <- metric_set(accuracy, recall, precision, f_meas)
registerDoParallel(cluster)
# Begin timing for this model
start_time <- Sys.time()
# Naive Bayes resample
nb_rs <- nb_model_wflow %>%
    tune_grid(
    resamples = severity_folds,
    metrics = perf_metrics,
    control = ctrl_obj
  )
# End time and get the difference 
end_time <- Sys.time()
end_time - start_time
nb_rs
stopCluster(cluster)
```
Step 4e: Performance Metrics
```{r}
nb_perf_metrics <- collect_metrics(nb_rs)
nb_perf_metrics
```
AutoPlot - show the performance profile across tuning parameters
```{r}
autoplot(nb_rs) +
  scale_color_viridis_d(direction = -1) +
  theme(legend.position = "top")
```
F1 Score
```{r}
show_best(nb_rs, metric = "f_meas")
```
Precision
```{r}
show_best(nb_rs, metric = "precision")
```
Recall
```{r}
show_best(nb_rs, metric = "recall")
```
Accuracy
```{r}
show_best(nb_rs, metric = "accuracy")
```

## Cleanup
```{r}
unlink("unzipfolder", recursive = TRUE)
```
